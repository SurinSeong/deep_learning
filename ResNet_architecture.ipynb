{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1nC+r06x8ETfNe1eb6ulF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurinSeong/deep_learning/blob/main/ResNet_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-18 구조 파악 (Pytorch)"
      ],
      "metadata": {
        "id": "J2taETYSAAex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHmxg84AkKEa"
      },
      "outputs": [],
      "source": [
        "## 7x7 conv layer로 시작\n",
        "# ResNet에 적합한 초기화 방법 : He initialization ==> 가중치 초기화\n",
        "## residual block을 깊게 쌓아 깊은 층을 가지도록 설계\n",
        "# 3x3 conv layer 사용 ==> 파라미터 수가 급격하게 증가하지 않도록 설계\n",
        "# block의 색이 변할 때마다 공간 해상도는 절반으로 감소하고 (strides=2), 채널의 수는 두배씩 증가\n",
        "## 하나의 FC layer로 최종 결과 값을 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 시작부분\n",
        "    * 7x7, 64, stride 2\n",
        "    * 3x3 max pool, stride 2"
      ],
      "metadata": {
        "id": "b4ZwJ8U-BgPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 ResNet 50층\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)"
      ],
      "metadata": {
        "id": "0RyGltdWDEdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    r\"\"\"\n",
        "    - pretrained: pretrained된 모델 가중치를 불러오기 (saved by caffe)\n",
        "    - arch: ResNet모델 이름\n",
        "    - block: 어떤 block 형태 사용할지 (\"Basic or Bottleneck\")\n",
        "    - layers: 해당 block이 몇번 사용되는지를 list형태로 넘겨주는 부분\n",
        "    \"\"\"\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "metadata": {
        "id": "6ZEVPtqvDKL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution Layer"
      ],
      "metadata": {
        "id": "zTo5O7GlDUu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    r\"\"\"\n",
        "    3x3 convolution with padding\n",
        "    - in_planes: in_channels\n",
        "    - out_channels: out_channels\n",
        "    - bias=False: BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정.\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "metadata": {
        "id": "ueQ3j9irDS99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bottleneck"
      ],
      "metadata": {
        "id": "6Sz2u8UDDdsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4 # 블록 내에서 차원을 증가시키는 3번째 conv layer에서의 확장계수\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        # ResNext나 WideResNet의 경우 사용\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "\n",
        "        # Bottleneck Block의 구조\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation) # conv2에서 downsample\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        # 1x1 convolution layer\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # 3x3 convolution layer\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        # 1x1 convolution layer\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        # skip connection\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "1hA8aWivDgsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet class"
      ],
      "metadata": {
        "id": "QLfqcjE5DgKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    # __init__에서 처음 conv1층과 마지막층(pooing과 fully connected) 이외에는 _make_layer함수로 모델의 제일 큰 단위의 층을 생성 및 정의\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        # default values\n",
        "        self.inplanes = 64 # input feature map\n",
        "        self.dilation = 1\n",
        "        # stride를 dilation으로 대체할지 선택\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        r\"\"\"\n",
        "        - 처음 입력에 적용되는 self.conv1과 self.bn1, self.relu는 모든 ResNet에서 동일\n",
        "        - 3: 입력으로 RGB 이미지를 사용하기 때문에 convolution layer에 들어오는 input의 channel 수는 3\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        r\"\"\"\n",
        "        - 아래부터 block 형태와 갯수가 ResNet층마다 변화\n",
        "        - self.layer1 ~ 4: 필터의 개수는 각 block들을 거치면서 증가(64->128->256->512)\n",
        "        - self.avgpool: 모든 block을 거친 후에는 Adaptive AvgPool2d를 적용하여 (n, 512, 1, 1)의 텐서로\n",
        "        - self.fc: 이후 fc layer를 연결\n",
        "        \"\"\"\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, # 여기서부터 downsampling적용\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    # _make_layer함수는 논문의 conv2_X, conv3_x, conv4_X, conv5_x을 구현하며 각 층에 해당하는 block을 갯수에 맞게 생성 및 연결시켜주는 역할\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        r\"\"\"\n",
        "        convolution layer 생성 함수\n",
        "        - block: block종류 지정\n",
        "        - planes: feature map size (input shape)\n",
        "        - blocks: layers[0]와 같이, 해당 블록이 몇개 생성돼야하는지, 블록의 갯수 (layer 반복해서 쌓는 개수)\n",
        "        - stride와 dilate은 고정\n",
        "        \"\"\"\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "\n",
        "        # the number of filters is doubled: self.inplanes와 planes 사이즈를 맞춰주기 위한 projection shortcut\n",
        "        # the feature map size is halved: stride=2로 downsampling\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        # 블록 내 시작 layer, downsampling 필요\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion # inplanes 업데이트\n",
        "        # 동일 블록 반복\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    # forward함수로 모델에 대한 feedforward를 진행\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)"
      ],
      "metadata": {
        "id": "7KReWH0aCUEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Transfer-Learning"
      ],
      "metadata": {
        "id": "GNW84UZIGNyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torchvision 관련 라이브러리 로드\n",
        "from torchvision import utils, datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ezSgbYOdGTep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 사용 준비\n",
        "* 사용할 데이터셋 : STL10\n",
        "    * Image Classification의 벤치마크로 10개의 라벨을 가진 데이터 셋\n",
        "    * torchvision에서는 5000개의 train 데이터와 8000개의 test로 구성되어있음."
      ],
      "metadata": {
        "id": "GyfLAtbeGvTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로 설정\n",
        "import os\n",
        "\n",
        "os.mkdir('./train')\n",
        "os.mkdir('./test')"
      ],
      "metadata": {
        "id": "83-KvlzPGopz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 데이터 다운 (datasets.STL10 메소드 다운)\n",
        "train_dataset = datasets.STL10('./train', split='train', download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.STL10('./test', split='test', download=True, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tS4SdvzHOsZ",
        "outputId": "9e997961-df45-4766-9a0f-123534ed9785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./train/stl10_binary.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2640397119/2640397119 [02:12<00:00, 19937300.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./train/stl10_binary.tar.gz to ./train\n",
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./test/stl10_binary.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2640397119/2640397119 [01:24<00:00, 31111430.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./test/stl10_binary.tar.gz to ./test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 다운 받은 이미지에 대해 스케일링 과정이 필요함.\n",
        "    * transform을 활용해 이미지 크기를 고정하고, normalization을 진행\n",
        "* 주어진 데이터셋의 이미지는 RGB 3개의 채널로 구성되어 있음."
      ],
      "metadata": {
        "id": "DSwYT8zvIBQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 채널별 mean값과 std값을 계산한 후, transform을 정의\n",
        "import numpy as np\n",
        "\n",
        "# 채널별 mean 계산\n",
        "def get_mean(dataset):\n",
        "    meanRGB = [np.mean(image.numpy(), axis=(1, 2)) for image, _ in dataset] # 각 image를 가져오고, _는 라벨을 무시하겠다는 의미 / image.numpy() : image는 pytorch의 Tensor 객체, pytorch 텐서를 numpy 배열로 변환하겠다는 의미 / axis : 높이(H)와 너비(W)에 대해 연산을 수행\n",
        "    meanR = np.mean([m[0] for m in meanRGB])\n",
        "    meanG = np.mean([m[1] for m in meanRGB])\n",
        "    meanB = np.mean([m[2] for m in meanRGB])\n",
        "    return [meanR, meanG, meanB]\n",
        "\n",
        "# 채널별 std 계산\n",
        "def get_std(dataset):\n",
        "    stdRGB = [np.std(image.numpy(), axis=(1, 2)) for image, _ in dataset]\n",
        "    stdR = np.mean([s[0] for s in stdRGB])\n",
        "    stdG = np.mean([s[1] for s in stdRGB])\n",
        "    stdB = np.mean([s[2] for s in stdRGB])\n",
        "    return [stdR, stdG, stdB]"
      ],
      "metadata": {
        "id": "BWjEiJ-CHo1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* augmentation 방법"
      ],
      "metadata": {
        "id": "GDWBamQfLWpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms.Compose 메소드로 transform(변형) 단계를 묶어서 진행\n",
        "# 이미지의 크기를 임의로 128으로 고정한 후, 정규화하는 과정만 진행\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize((128, 128)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(get_mean(train_dataset), get_std(train_dataset))])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize((128, 128)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(get_mean(test_dataset), get_std(test_dataset))])\n",
        "\n",
        "# transform 정의\n",
        "train_dataset.transform = train_transforms\n",
        "test_dataset.transform = test_transforms"
      ],
      "metadata": {
        "id": "1ZSOjV-XK8bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* dataloader 정의"
      ],
      "metadata": {
        "id": "KS-2nuzcMHNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader 정의\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "IfvC-iYlMDLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "* 모델 학습을 위해 pretrained된 resnet50 모델을 사용\n",
        "    * 해당 resnet 모델은 사전 학습된 모델로, 이미지 분류 문제를 해결할 수 있도록 규모가 큰 데이터(ImageNet)로 미리 학습된 모델이다."
      ],
      "metadata": {
        "id": "pd--DwT0MlCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "# 학습 환경 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# true 옵션으로 사전 학습된 모델을 로드\n",
        "model = models.resnet50(pretrained=True).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwy9Zfr0MgMb",
        "outputId": "5316b955-a6a4-4cb8-c94e-c1ed8b615742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 120MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* torchsummary의 summary 메소드로 모델을 요약해서 확인 가능\n",
        "* 사용할 데이터셋은 RGB 3개의 channel로 구성"
      ],
      "metadata": {
        "id": "IUS0bn2cNjSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 layer별 파라미터 개수 확인\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (3, 128, 128))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j32FOdA-NaTN",
        "outputId": "8f23912c-0786-435c-bf1f-c19893d9e52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
            "              ReLU-3           [-1, 64, 64, 64]               0\n",
            "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5           [-1, 64, 32, 32]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "              ReLU-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "             ReLU-10           [-1, 64, 32, 32]               0\n",
            "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
            "           Conv2d-13          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 32]             512\n",
            "             ReLU-15          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-16          [-1, 256, 32, 32]               0\n",
            "           Conv2d-17           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 32, 32]             128\n",
            "             ReLU-19           [-1, 64, 32, 32]               0\n",
            "           Conv2d-20           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 32, 32]             128\n",
            "             ReLU-22           [-1, 64, 32, 32]               0\n",
            "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
            "             ReLU-25          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-26          [-1, 256, 32, 32]               0\n",
            "           Conv2d-27           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 32, 32]             128\n",
            "             ReLU-29           [-1, 64, 32, 32]               0\n",
            "           Conv2d-30           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 32, 32]             128\n",
            "             ReLU-32           [-1, 64, 32, 32]               0\n",
            "           Conv2d-33          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 32, 32]             512\n",
            "             ReLU-35          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-36          [-1, 256, 32, 32]               0\n",
            "           Conv2d-37          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
            "             ReLU-39          [-1, 128, 32, 32]               0\n",
            "           Conv2d-40          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 16, 16]             256\n",
            "             ReLU-42          [-1, 128, 16, 16]               0\n",
            "           Conv2d-43          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-45          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-47          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
            "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
            "             ReLU-51          [-1, 128, 16, 16]               0\n",
            "           Conv2d-52          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 16, 16]             256\n",
            "             ReLU-54          [-1, 128, 16, 16]               0\n",
            "           Conv2d-55          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-57          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-58          [-1, 512, 16, 16]               0\n",
            "           Conv2d-59          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 16, 16]             256\n",
            "             ReLU-61          [-1, 128, 16, 16]               0\n",
            "           Conv2d-62          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 16, 16]             256\n",
            "             ReLU-64          [-1, 128, 16, 16]               0\n",
            "           Conv2d-65          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-67          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-68          [-1, 512, 16, 16]               0\n",
            "           Conv2d-69          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 16, 16]             256\n",
            "             ReLU-71          [-1, 128, 16, 16]               0\n",
            "           Conv2d-72          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 16, 16]             256\n",
            "             ReLU-74          [-1, 128, 16, 16]               0\n",
            "           Conv2d-75          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-77          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-78          [-1, 512, 16, 16]               0\n",
            "           Conv2d-79          [-1, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
            "             ReLU-81          [-1, 256, 16, 16]               0\n",
            "           Conv2d-82            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 8, 8]             512\n",
            "             ReLU-84            [-1, 256, 8, 8]               0\n",
            "           Conv2d-85           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
            "           Conv2d-87           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-88           [-1, 1024, 8, 8]           2,048\n",
            "             ReLU-89           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-90           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-91            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-92            [-1, 256, 8, 8]             512\n",
            "             ReLU-93            [-1, 256, 8, 8]               0\n",
            "           Conv2d-94            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-95            [-1, 256, 8, 8]             512\n",
            "             ReLU-96            [-1, 256, 8, 8]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-101            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 8, 8]             512\n",
            "            ReLU-103            [-1, 256, 8, 8]               0\n",
            "          Conv2d-104            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 8, 8]             512\n",
            "            ReLU-106            [-1, 256, 8, 8]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-111            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 8, 8]             512\n",
            "            ReLU-113            [-1, 256, 8, 8]               0\n",
            "          Conv2d-114            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 8, 8]             512\n",
            "            ReLU-116            [-1, 256, 8, 8]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-121            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 8, 8]             512\n",
            "            ReLU-123            [-1, 256, 8, 8]               0\n",
            "          Conv2d-124            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 8, 8]             512\n",
            "            ReLU-126            [-1, 256, 8, 8]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-131            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 8, 8]             512\n",
            "            ReLU-133            [-1, 256, 8, 8]               0\n",
            "          Conv2d-134            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 8, 8]             512\n",
            "            ReLU-136            [-1, 256, 8, 8]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-141            [-1, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-142            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-143            [-1, 512, 8, 8]               0\n",
            "          Conv2d-144            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-146            [-1, 512, 4, 4]               0\n",
            "          Conv2d-147           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 4, 4]           4,096\n",
            "          Conv2d-149           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-151           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-152           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-153            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-155            [-1, 512, 4, 4]               0\n",
            "          Conv2d-156            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-158            [-1, 512, 4, 4]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-161           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-162           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-163            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-165            [-1, 512, 4, 4]               0\n",
            "          Conv2d-166            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-168            [-1, 512, 4, 4]               0\n",
            "          Conv2d-169           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-171           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-172           [-1, 2048, 4, 4]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                 [-1, 1000]       2,049,000\n",
            "================================================================\n",
            "Total params: 25,557,032\n",
            "Trainable params: 25,557,032\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 93.59\n",
            "Params size (MB): 97.49\n",
            "Estimated Total Size (MB): 191.27\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습을 위한 함수 정의\n",
        "* 가장 간단한 형태의 train 컨테이너를 구성"
      ],
      "metadata": {
        "id": "LZI9Q6c5OD8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim"
      ],
      "metadata": {
        "id": "panbDnE0N0eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 전이학습을 위해 필요한 파라미터 정의\n",
        "    * lr = 0.0001\n",
        "    * 많은 epoch의 학습을 진행하 때에는 스케쥴러를 사용하면 용이하지만, 가장 간단한 train 컨테이너를 구성해 다루지 않음.\n",
        "    * optimizer='adam'\n",
        "    * loss function='crossentropy' (학습 목적에 따라 다양하게 구성 가능)\n",
        "    * num_epochs(학습횟수)=5"
      ],
      "metadata": {
        "id": "2UvcNlZ5OQUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 설정 ==> 수정 예정\n",
        "\n",
        "lr = 0.0001\n",
        "num_epochs = 5\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "loss_function = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "rE-aV0Y7OO7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 저장\n",
        "params = \\\n",
        " {'num_epochs':num_epochs,\n",
        "  'optimizer':optimizer,\n",
        "  'loss_function':loss_function,\n",
        "  'train_dataloader':train_dataloader,\n",
        "  'test_dataloader':test_dataloader,\n",
        "  'device':device}"
      ],
      "metadata": {
        "id": "n4WJDehWO4mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 함수\n",
        "def train(model, params):\n",
        "    # 손실함수\n",
        "    loss_function=params['loss_function']\n",
        "    # 훈련 데이터 로더\n",
        "    train_dataloader=params['train_dataloader']\n",
        "    # 테스트 데이터 로더\n",
        "    test_dataloader=params['test_dataloader']\n",
        "    # 학습환경\n",
        "    device=params['device']\n",
        "\n",
        "    for epoch in range(0, num_epochs):\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            # train dataloader로 불러온 데이터에서 이미지와 라벨을 분리한다.\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # 이전 batch에서 계산된 가중치를 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + back propagation 연산 (순전파, 역전파)\n",
        "            # 사전학습된 모델에 input 넣어서 output 출력\n",
        "            outputs = model(inputs)\n",
        "            train_loss = loss_function(outputs, labels)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # test accuracy 계산\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        accuracy = []\n",
        "\n",
        "        for i, data in enumerate(test_dataloader, 0):\n",
        "            # test dataloader로 불러온 데이터에서 이미지와 라벨을 분리한다.\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # 결과값 연산\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            test_loss = loss_function(outputs, labels).item()\n",
        "            accuracy.append(100 * correct/total)\n",
        "\n",
        "        # 학습 결과 출력\n",
        "        print('Epoch: %d/%d, Train loss: %.6f, Test loss: %.6f, Accuracy: %.2f' %(epoch+1, num_epochs, train_loss.item(), test_loss, 100*correct/total))"
      ],
      "metadata": {
        "id": "NLsGGcjVPSZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "StXrKSkbSGrD",
        "outputId": "b8611392-f2bc-47f0-bcb4-2f8fa3a0d255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5, Train loss: 0.927852, Test loss: 0.438244, Accuracy: 90.28\n",
            "Epoch: 2/5, Train loss: 1.955365, Test loss: 0.535371, Accuracy: 90.34\n",
            "Epoch: 3/5, Train loss: 0.607415, Test loss: 0.387795, Accuracy: 90.04\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-729370a26069>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-c343e7954756>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eho1JB1OSI9b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}